# -*- coding: utf-8 -*-
"""ISIL_ELMASOGLU_INCOME_CASE_STUDY

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OYGaJAEuYh7iheAUtet0P_WgY7AhJnd1
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function #libraries
import pandas as pd
import numpy as np
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as plt
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union
from sklearn.naive_bayes import MultinomialNB
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, IterativeImputer
from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier,RandomForestClassifier,ExtraTreesClassifier
from pandas import set_option
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import VotingClassifier, BaggingClassifier
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,cross_val_score,GridSearchCV,learning_curve,RandomizedSearchCV
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report,accuracy_score, f1_score,r2_score,roc_auc_score
from sklearn.linear_model import LogisticRegression, BayesianRidge,Lasso, ElasticNet
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from imblearn.under_sampling import RandomUnderSampler,TomekLinks
from imblearn.over_sampling import SMOTE
from sklearn.impute import KNNImputer
from lightgbm import LGBMClassifier
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore") # Don't want to see the warnings in the notebook
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
   
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt

"""The code below indicates KNN imputation method for missing values on target and has model results. If multi-class classification will be tried, need to open codes which are under comment below, there is a guide with comments."""

df = pd.read_csv("income_case_study.csv",sep=";") #read csv file with delimiter

df.income.value_counts() #target values

df.income.isnull().sum() #missing data on target exist

df.info()#45222 entries , features

""""df.loc[df['income'] =='>50K', 'income'] = 1 #imputing income with 0 if <=50K,>50K 1 and nan values with 2. Run this part when trying to create multi-class classification model. 
df.loc[df['income'] == '<=50K', 'income'] = 0
df.loc[df['income'].isnull(),'income'] = 2
df.income=df.income.astype('int')
df""""

df.income.value_counts()

df.loc[df['income'] =='>50K', 'income'] = 1 #Without eliminating Nan values, we hold them for KNN imputation.
df.loc[df['income'] == '<=50K', 'income'] = 0
df

df.isnull().sum().sort_values(ascending = False) #CHECK MISSING VALUES

df.describe()

sns.countplot(df.income,palette = 'Dark2_r') #unbalanced data set, 0 is 2.5 times more than 1

df.isnull().sum().sort_values(ascending = False) #CHECK MISSING VALUES

df.describe() #Checking numerical features for statistics,according to the data missing values of age,education-num and hours-per-week should be filled with mean,
#capital-gain	and capital-loss should be median, because of the standard deviation and variance of data. But only age,education-num and education_level have missing values, so filling with mean is enough.

sns.pairplot(df) #check relationships between features

cor_mat=df.corr() #correlation matrix for numerical features, if there are related two features exist. But in our set we dont have highly correlated two variables.
sns.heatmap(cor_mat)
plt.show()

df["Name"].unique() #uniqueness control

df.occupation.unique() #uniqueness control

df.drop_duplicates(inplace=True) #checking and erasing duplicates

df.info() #43102 rows left, because of duplicates removed

df = pd.concat((df,pd.get_dummies(df['native-country'], drop_first=True)),axis=1) #pd.concat for categorical features converted to numerical features 
df = pd.concat((df,pd.get_dummies(df['workclass'], drop_first=True)),axis=1) 
df = pd.concat((df,pd.get_dummies(df['education_level'], drop_first=True)),axis=1)
df = pd.concat((df,pd.get_dummies(df['marital-status'], drop_first=True)),axis=1) 
df = pd.concat((df,pd.get_dummies(df['occupation'], drop_first=True)),axis=1)
df = pd.concat((df,pd.get_dummies(df['relationship'], drop_first=True)),axis=1) 
df = pd.concat((df,pd.get_dummies(df['race'], drop_first=True)),axis=1)
df = pd.concat((df,pd.get_dummies(df['sex'], drop_first=True)),axis=1)
df.drop(["native-country","education_level","occupation","workclass","marital-status","relationship","race","sex"],axis=1,inplace=True) #dropping old categorical features

df.head()



"""mice_impute = IterativeImputer(estimator=RandomForestClassifier(max_depth=7),initial_strategy='median') #we tried iterative imputer for imputing target column, it has the same results like knnimputer below.
df1["income2"]  = mice_impute.fit_transform(df1[["income"]] )"""

imputer = KNNImputer() #defining knn imputer
df["income2"] = imputer.fit_transform(df[["income"]])   #for income column we use knn imputation
df.income2.value_counts()

df.income=pd.Series(np.where(df.income2.values >= 0.75, 1, 0),df.index) #if values converges to 1 fill with them 1, else fill with 0.
df.drop(columns=['income2'],axis=1,inplace=True) #in our case values converges to 0, so missing values will be filled with 0 even though its not a good decision for filling. With the data on hand, missing values doesnt show a good pattern.
df.income.value_counts()

df1 = df.sample(frac=0.80,random_state=42) #train-test set split
df1
test=df.drop(df1.index)

X=df1.drop(columns=['income'],axis=1) #DELETE TARGET
y=df1['income']
X.head()

sns.countplot(df1['income'], palette='Set3') #CHECK TARGET VALUES' BALANCE

X.describe(include="object") #only name column left as categorical, as its personal data we can drop this feature

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42,stratify=y) #Train validation split data for the model, with stratify option we preserved train /validation target proportion for unbalanced data.
X_train.head()

catColumnsToDrop = ["Name"] #categorical columns to drop
numColumnsToDrop = [] #numerical columns to drop

# compose global one hot encoder
allData = pd.concat((X, test), axis=0) #onehotencoding for all data
allData_cat = allData.select_dtypes(include="object")
allData_cat.drop(catColumnsToDrop, axis=1, inplace=True)

simp = SimpleImputer(strategy="most_frequent") 
#allData_cat = simp.fit_transform(allData_cat)  #there is no categorical feature left, because of that we wont use fit_transform

encoder = OneHotEncoder()
#encoder.fit(allData_cat)

class handleNumericFeatures(TransformerMixin): #erasing unnecessary numeric columns and filling missing data with mean
  def __init__(self, numColumnsToDrop):
    self.numColumnsToDrop = numColumnsToDrop

  def fit(self, X, y=None):
    X_num = X.select_dtypes(exclude="object").drop(self.numColumnsToDrop, axis=1) 
    simp = SimpleImputer(strategy="mean")
    simp.fit(X_num)
    self.simp = simp
    return self

  def transform(self, X, y=None):
    X_num = X.select_dtypes(exclude="object").drop(self.numColumnsToDrop, axis=1)
    X_num = self.simp.transform(X_num)
    return pd.DataFrame(X_num)

class handleCategoricFeatures(TransformerMixin): #CAT COLS DROP AND IMPUTE WITH MOST FREQUENT VALUES AND ONEHOTENCODING
  def __init__(self, encoder, catColumnsToDrop):
    self.encoder = encoder
    self.catColumnsToDrop = catColumnsToDrop

  def fit(self, X, y=None):
    X_cat = X.select_dtypes(include="object").drop(self.catColumnsToDrop, axis=1) #CHOOSE CAT COLS, DROP CAT COLS TO DROP
    simp = SimpleImputer(strategy="most_frequent") #FILL WITH MOST FREQUENT VALUE
    simp.fit(X_cat)
    self.simp = simp
    return self
  
  def transform(self, X, y=None):
    X_cat = X.select_dtypes(include="object").drop(self.catColumnsToDrop, axis=1) 
    X_cat = self.simp.transform(X_cat)
    res = self.encoder.transform(X_cat) #APPLY ONE HOT ENCODING
    return pd.DataFrame(res.toarray())

union = FeatureUnion([
      ("numeric", handleNumericFeatures(numColumnsToDrop=numColumnsToDrop)),
     # ("categoric", handleCategoricFeatures(encoder=encoder, catColumnsToDrop=catColumnsToDrop)) we dont need this statements, because we handle categoric features with pd.getdummies()
])
union.fit_transform(X_train).shape

#PRINT THE FILLED AND ENCODED DATA
X_a=union.fit_transform(X_train)
X_a=pd.DataFrame(X_a) 
X_a.head()

pd.Series(y).value_counts().max()/pd.Series(y).value_counts().sum() #base accuracy

pipe = Pipeline([   #model testing with different parameters, trying to find out the best parameter for Random Forest Classifier
    ("union", union), #calling the classes
    ("imputer", SimpleImputer(strategy="mean")), #imputing data with mean
    ("scaler", StandardScaler()), #scaling features to prevent the differences/big multipliers between variables, we dont want a dominant variable, because of that scaling is needed.
    ("rnd_clf", RandomForestClassifier(random_state=42, n_jobs=-1))
])

params = {
    'rnd_clf__n_estimators': [400, 500],
    'rnd_clf__max_depth': [3, 4, 5],
    'rnd_clf__max_features': [0.5, 0.6, 0.4]
}

grid = GridSearchCV(pipe, params, cv=3) #cross validation to prevent overfitting and better performing model, GridSearchCV is used for findind the best parameters for the model
grid.fit(X_train, y_train)

print(grid.best_score_)
print(grid.best_params_) #finding the best parameters.

plot_learning_curve(grid.best_estimator_, "RandomFrst", X_train, y_train, cv=3) #plotting learning curve

#plot graph of feature importances for better visualization
X_traingraph=X_train.drop('Name',axis=1)
cols = list(X_traingraph.columns)
feat_importances = pd.Series(grid.best_estimator_.named_steps["rnd_clf"].feature_importances_, index=cols)
feat_importances.nlargest(10).plot(kind='barh')
plt.show() #graph shows that top 10 important features for our model.

#re-entering model with best features, here is a class for feature selection
arr = grid.best_estimator_.named_steps["rnd_clf"].feature_importances_
importantColIndexes = arr.argsort()[-7:][::-1]
importantColIndexes
class selectFeatures(TransformerMixin):
  def __init__(self, importantColIndexes):
    self.importantColIndexes = importantColIndexes
  
  def fit(self, X, y=None):
    return self
  
  def transform(self, X, y=None):
    return pd.DataFrame(X).iloc[:,self.importantColIndexes]

pipe_featureSelection = Pipeline([
      ("union", union),
      ("imputer", SimpleImputer(strategy="mean")),
      ("scaler", StandardScaler()),
      ("featureSelect", selectFeatures(importantColIndexes=importantColIndexes)), #using selected features 
      ("rnd_clf", RandomForestClassifier(random_state=42, n_jobs=-1,max_features = 0.6, max_depth= 5, n_estimators= 500)) #choosing the best parameters.
])


pipe_featureSelection.fit(X_train, y_train) #fitting the data
y_pred = pipe_featureSelection.predict(X_val) #making predictions
accuracy_score(y_val, y_pred) #accuracy score metric for prediction on train set

plot_learning_curve(pipe_featureSelection, "random", X_train, y_train, cv=3) #learning curves look better

pipe = Pipeline([ #trying with another model, XGBoost modeling . Trying to find best parameters with GridSearchCV again.
    ("union", union),
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler()),
    ("xgb", XGBClassifier(random_state=42))
])

params = {
    'xgb__n_estimators': [50, 100],
    'xgb__max_depth': [2, 3],
    'xgb__subsample': [0.5, 0.7],
    'xgb__colsample_bytree': [0.5, 0.7],
    'xgb__gamma': [0, 1, 5]
}

grid = GridSearchCV(pipe, params, cv=3)
grid.fit(X_train, y_train)
print(grid.best_score_)
print(grid.best_params_)
plot_learning_curve(grid.best_estimator_, "XGB", X_train, y_train, cv=3) #trying xg boosting method as model with best parameters, graph is not very stable

pipe_xgb= Pipeline([
    ("union", union),
    ("imputer", SimpleImputer(strategy="mean")), 
    ("scaler", StandardScaler()),
    ("xgb", XGBClassifier(random_state=42,colsample_bytree=0.5,gamma=0,max_depth=3,n_estimators=100,subsample=0.5 ))  #for multi class, we added objective='softmax' parameter before running.
])

pipe_xgb.fit(X_train, y_train)
y_pred = pipe_xgb.predict(X_val)
accuracy_score(y_val, y_pred)

a = pd.DataFrame(pd.Series(y_pred))
a.value_counts() #predictions

feat_importances = pd.Series(pipe_xgb.named_steps["xgb"].feature_importances_, index=cols)
feat_importances.nlargest(10).plot(kind='barh')
plt.show() #feature importances for XGB model

pipe_knn= Pipeline([
    ("union", union),
    ("imputer", SimpleImputer(strategy="mean")), 
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier())
])

pipe_knn.fit(X_train, y_train)
y_pred = pipe_knn.predict(X_val)
accuracy_score(y_val, y_pred) #KNN model

a = pd.DataFrame(pd.Series(y_pred))
a.value_counts()

plot_learning_curve(pipe_knn, "knn", X_train, y_train, cv=3) #underfitting problems, couldnt learn the train data well with KNN model

pipe_lgbm = Pipeline([ #trying LGBM model
    ("union", union),
    ("imputer", SimpleImputer(strategy="mean")), 
    ("scaler", StandardScaler()),
    ("lgbm", LGBMClassifier(random_state=42, n_jobs=-1)) #objective='multiclass' should be added for multi class solution before running.
])

pipe_lgbm.fit(X_train, y_train)
y_pred = pipe_lgbm.predict(X_val)
accuracy_score(y_val, y_pred) #lgbm has the best accuracy until now as it seems with train set

a = pd.DataFrame(pd.Series(y_pred))
a.value_counts()

plot_learning_curve(pipe_lgbm, "lgbm", X_train, y_train, cv=3) #learning curves has a good graph

X_traingraph=X_train.drop('Name',axis=1)
cols = list(X_traingraph.columns)
feat_importances = pd.Series(pipe_lgbm.named_steps["lgbm"].feature_importances_, index=cols)
feat_importances.nlargest(10).plot(kind='barh')
plt.show() #lgbm important features

cr=classification_report(y_val,y_pred) #model performance metrics f1,accuracy, precision,recall for LGBM
print(cr)

f1=f1_score(y_val,y_pred)
print(f1) #printing f1 score
recall=recall_score(y_val,y_pred)
print(recall) #printing recall score

cm = confusion_matrix(y_true=y_val, y_pred=y_pred) #diagonally tp and tn are important 6096 and 1340, confusion matrix
print(cm)

testpredict=test.drop(columns="income",axis=1) #predict for unseen data , drop the real income value on the set

# fit model with whole data
pipe_lgbm.fit(X, y) #choose the best model , we use LGBM
y_pred = pipe_lgbm.predict(testpredict) #MAKE PREDICTIONS
a = pd.DataFrame(pd.Series(y_pred))

a['index']=a.index+1
a["income"]=a[0]
a.drop(0,axis=1,inplace=True)

a.income.value_counts()

test=test.reset_index() #test data indexing to compare with our predictions
test["realincome"]=test.income
test1=test.drop(columns="income",axis=1)
test1=test1[["realincome"]]
test1

test1.realincome.value_counts() #real income values

comparedf=pd.concat((a, test1), axis=1) #compare df for our predictions income is our predictions, realincome test data's original income value
comparedf

comparedf[(comparedf["income"] == comparedf["realincome"])]

comparedf.info()

comparedf["realincome"]= pd.to_numeric(comparedf["realincome"], errors='coerce')

cr=classification_report(comparedf.realincome,comparedf.income) 
print(cr) #real test results

cm = confusion_matrix(y_true=comparedf.realincome, y_pred=comparedf.income) #real results for confusion matrix
print(cm)

f1=f1_score(comparedf.realincome,comparedf.income)
print(f1)
recall=recall_score(comparedf.realincome,comparedf.income)
print(recall)

precision=precision_score(comparedf.realincome,comparedf.income) #results are showing good fit for our predictions, its parallel to what we have found on train/validation metrics. 
print(precision)

sns.countplot(a['income'], palette='Set3') #CHECK THE TARGET WITHIN TEST DATA BALANCE

a.to_excel("result.xlsx", index=False, header=True) #to print as excel file for predictions

"""These results are showing that when we use KNN imputation for target set, it's showing a good fit with metrics, but as we assume and impute value for target it causes data leakage and we are not sure about real target values for NaN values. When we try with multi-class classification model(define another class for Nan like, 0 ,1,2 for NAN values) it couldnt detect nan class well even though we tried different models and methods. This shows that we need more data to understand the pattern of missing values, other classes >50K and <=50K predicted well with the models."""

